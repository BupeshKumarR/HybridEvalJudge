# Usage Guide

Complete guide to using the LLM Judge Auditor toolkit for evaluating AI-generated text.

## Table of Contents

1. [Getting Started](#getting-started)
2. [Basic Evaluation](#basic-evaluation)
3. [Batch Processing](#batch-processing)
4. [Configuration](#configuration)
5. [Understanding Results](#understanding-results)
6. [Advanced Features](#advanced-features)
7. [Best Practices](#best-practices)
8. [Troubleshooting](#troubleshooting)

## Getting Started

### Prerequisites

Before using the toolkit, ensure you have:

1. **Activated the virtual environment**:
   ```bash
   source .venv/bin/activate  # macOS/Linux
   # or
   .venv\Scripts\activate.bat  # Windows
   ```

2. **Verified installation**:
   ```bash
   pytest  # All tests should pass
   ```

### First Steps

Start with the simple evaluation example:

```bash
python examples/simple_evaluation.py
```

This will walk you through a basic evaluation and show you the key concepts.

## Basic Evaluation

### Minimal Example

```python
from llm_judge_auditor import EvaluationToolkit

# Initialize toolkit
toolkit = EvaluationToolkit.from_preset("fast")

# Evaluate
result = toolkit.evaluate(
    source_text="Paris is the capital of France.",
    candidate_output="Paris is the capital of France."
)

# View score
print(f"Score: {result.consensus_score:.2f}/100")
```

### Understanding the Inputs

**Source Text**: The reference text containing facts or context
- Can be a document, article, or any factual content
- Used as the ground truth for verification

**Candidate Output**: The text to be evaluated
- Typically generated by an LLM
- Evaluated for factual accuracy against the source

### Choosing a Preset

The toolkit includes four presets optimized for different use cases:

```python
# Fast: Minimal resources, quick evaluation
toolkit = EvaluationToolkit.from_preset("fast")

# Balanced: Good accuracy/speed tradeoff (recommended)
toolkit = EvaluationToolkit.from_preset("balanced")

# Strict: Maximum accuracy, all features enabled
toolkit = EvaluationToolkit.from_preset("strict")

# Research: Full transparency and metrics
toolkit = EvaluationToolkit.from_preset("research")
```

**Preset Comparison**:

| Preset | Judges | Retrieval | Quantization | Use Case |
|--------|--------|-----------|--------------|----------|
| fast | 1 | No | Yes | Quick checks, development |
| balanced | 2 | Yes | Yes | Production use |
| strict | 3 | Yes | Yes | High-stakes evaluation |
| research | 3 | Yes | No | Research, benchmarking |

## Batch Processing

### Basic Batch Evaluation

```python
from llm_judge_auditor import EvaluationToolkit, EvaluationRequest

# Initialize toolkit
toolkit = EvaluationToolkit.from_preset("balanced")

# Create requests
requests = [
    EvaluationRequest(
        source_text="Source 1...",
        candidate_output="Candidate 1...",
        task="factual_accuracy"
    ),
    EvaluationRequest(
        source_text="Source 2...",
        candidate_output="Candidate 2...",
        task="factual_accuracy"
    ),
]

# Process batch
batch_result = toolkit.batch_evaluate(
    requests=requests,
    continue_on_error=True  # Continue if some evaluations fail
)

# View statistics
print(f"Mean score: {batch_result.statistics['mean']:.2f}")
print(f"Success rate: {batch_result.metadata['success_rate']:.1%}")
```

### Error Handling in Batches

```python
# Process batch with error resilience
batch_result = toolkit.batch_evaluate(
    requests=requests,
    continue_on_error=True
)

# Check for errors
if batch_result.errors:
    print(f"Errors occurred: {len(batch_result.errors)}")
    for error in batch_result.errors:
        print(f"Request {error['request_index']}: {error['error_message']}")
```

### Saving Batch Results

```python
# Save to JSON
batch_result.save_to_file("results.json")

# Or export to different formats
json_data = batch_result.to_json()
csv_data = batch_result.to_csv()
```

## Configuration

### Using Presets

```python
from llm_judge_auditor import ToolkitConfig

# Load preset
config = ToolkitConfig.from_preset("balanced")

# Customize preset
config.batch_size = 4
config.enable_retrieval = False

# Use customized config
toolkit = EvaluationToolkit(config)
```

### Custom Configuration

```python
from llm_judge_auditor import ToolkitConfig, AggregationStrategy

config = ToolkitConfig(
    # Model selection
    verifier_model="MiniCheck/flan-t5-base-finetuned",
    judge_models=[
        "microsoft/Phi-3-mini-4k-instruct",
        "mistralai/Mistral-7B-v0.1"
    ],
    
    # Hardware optimization
    quantize=True,
    device="auto",  # Auto-detect best device
    
    # Retrieval settings
    enable_retrieval=True,
    retrieval_top_k=3,
    
    # Aggregation strategy
    aggregation_strategy=AggregationStrategy.WEIGHTED_AVERAGE,
    judge_weights={
        "Phi-3": 0.6,
        "Mistral": 0.4
    },
    
    # Performance tuning
    batch_size=2,
    max_length=1024,
)

toolkit = EvaluationToolkit(config)
```

### Loading from YAML

```yaml
# config/my_config.yaml
verifier_model: "MiniCheck/flan-t5-base-finetuned"
judge_models:
  - "microsoft/Phi-3-mini-4k-instruct"
quantize: true
enable_retrieval: false
aggregation_strategy: "mean"
```

```python
config = ToolkitConfig.from_yaml("config/my_config.yaml")
toolkit = EvaluationToolkit(config)
```

## Understanding Results

### Result Structure

```python
result = toolkit.evaluate(source_text, candidate_output)

# Main score (0-100)
result.consensus_score  # Aggregated score from all judges

# Individual components
result.verifier_verdicts  # Statement-level fact-checking
result.judge_results      # Individual judge evaluations
result.flagged_issues     # Detected problems

# Report metadata
result.report.confidence          # Confidence in the evaluation
result.report.disagreement_level  # Judge disagreement metric
result.report.individual_scores   # Per-judge scores
```

### Interpreting Scores

**Consensus Score (0-100)**:
- **90-100**: Highly accurate, minimal issues
- **70-89**: Generally accurate, minor issues
- **50-69**: Moderate accuracy, some hallucinations
- **30-49**: Low accuracy, significant issues
- **0-29**: Very low accuracy, major hallucinations

**Confidence Level**:
- **High (>0.8)**: Judges agree, reliable evaluation
- **Medium (0.5-0.8)**: Some disagreement, reasonable confidence
- **Low (<0.5)**: High disagreement, uncertain evaluation

### Verifier Verdicts

```python
for verdict in result.verifier_verdicts:
    print(f"Claim: {verdict.claim}")
    print(f"Label: {verdict.label.value}")  # SUPPORTED, REFUTED, NOT_ENOUGH_INFO
    print(f"Confidence: {verdict.confidence:.2f}")
    print(f"Reasoning: {verdict.reasoning}")
```

### Flagged Issues

```python
for issue in result.flagged_issues:
    print(f"Type: {issue.type.value}")        # hallucination, bias, inconsistency
    print(f"Severity: {issue.severity.value}") # low, medium, high
    print(f"Description: {issue.description}")
    print(f"Evidence: {issue.evidence}")
```

### Hallucination Categories

```python
# View categorized hallucinations
for category, count in result.report.hallucination_categories.items():
    if count > 0:
        print(f"{category}: {count}")

# Categories include:
# - factual_error
# - unsupported_claim
# - temporal_inconsistency
# - numerical_error
```

## Advanced Features

### Retrieval-Augmented Verification

```python
config = ToolkitConfig.from_preset("balanced")
config.enable_retrieval = True
config.knowledge_base_path = "/path/to/wikipedia/dump"
config.retrieval_top_k = 5

toolkit = EvaluationToolkit(config)

# Evaluation will now use external knowledge
result = toolkit.evaluate(source_text, candidate_output)

# View retrieval provenance
for passage in result.report.retrieval_provenance:
    print(f"Source: {passage.source}")
    print(f"Relevance: {passage.relevance_score:.2f}")
    print(f"Text: {passage.text[:100]}...")
```

### Custom Evaluation Criteria

```python
from llm_judge_auditor import EvaluationRequest

request = EvaluationRequest(
    source_text=source_text,
    candidate_output=candidate_output,
    task="factual_accuracy",
    criteria=["correctness", "completeness", "relevance"]
)

result = toolkit.evaluate_request(request)

# View scores for each criterion
for criterion, score in result.criterion_scores.items():
    print(f"{criterion}: {score:.2f}")
```

### Bias Detection

```python
request = EvaluationRequest(
    source_text=source_text,
    candidate_output=candidate_output,
    task="bias_detection"
)

result = toolkit.evaluate_request(request)

# View bias findings
for issue in result.flagged_issues:
    if issue.type.value == "bias":
        print(f"[{issue.severity.value}] {issue.description}")
```

### Pairwise Comparison

```python
from llm_judge_auditor import PairwiseRequest

request = PairwiseRequest(
    source_text=source_text,
    candidate_a="First candidate output...",
    candidate_b="Second candidate output..."
)

result = toolkit.pairwise_compare(request)

print(f"Winner: {result.winner}")  # "A", "B", or "TIE"
print(f"Reasoning: {result.reasoning}")
```

## Best Practices

### 1. Choose the Right Preset

- **Development/Testing**: Use `fast` preset
- **Production**: Use `balanced` preset
- **Critical Applications**: Use `strict` preset
- **Research/Benchmarking**: Use `research` preset

### 2. Handle Errors Gracefully

```python
try:
    result = toolkit.evaluate(source_text, candidate_output)
except Exception as e:
    logger.error(f"Evaluation failed: {e}")
    # Implement fallback or retry logic
```

### 3. Monitor Confidence Levels

```python
if result.report.confidence < 0.5:
    logger.warning("Low confidence evaluation - judges disagree")
    # Consider manual review or re-evaluation
```

### 4. Use Batch Processing for Efficiency

```python
# More efficient than individual evaluations
batch_result = toolkit.batch_evaluate(requests)
```

### 5. Save Results for Analysis

```python
# Save individual results
result.save_to_file(f"result_{timestamp}.json")

# Save batch results
batch_result.save_to_file(f"batch_{timestamp}.json")
```

### 6. Optimize for Your Hardware

```python
config = ToolkitConfig.from_preset("balanced")

# For limited memory
config.quantize = True
config.batch_size = 1

# For GPU acceleration
config.device = "cuda"
config.batch_size = 4

# For Apple Silicon
config.device = "mps"
```

## Troubleshooting

### Issue: Out of Memory

**Solution**: Enable quantization and reduce batch size

```python
config.quantize = True
config.batch_size = 1
config.max_length = 512
```

### Issue: Slow Evaluation

**Solution**: Use faster preset or disable retrieval

```python
toolkit = EvaluationToolkit.from_preset("fast")
# or
config.enable_retrieval = False
```

### Issue: Low Confidence Scores

**Solution**: Use more judges or stricter preset

```python
config = ToolkitConfig.from_preset("strict")
# or
config.judge_models = ["model1", "model2", "model3"]
```

### Issue: Model Download Failures

**Solution**: Check network connection and disk space

```bash
# Check available disk space
df -h

# Manually download models
python -c "from llm_judge_auditor.components import ModelDownloader; ModelDownloader().download('model-name')"
```

### Issue: Inconsistent Results

**Solution**: Check for randomness in model inference

```python
# Set seed for reproducibility
import torch
torch.manual_seed(42)

# Use deterministic aggregation
config.aggregation_strategy = AggregationStrategy.MEAN
```

## Next Steps

- **Explore Examples**: Check `examples/` directory for more use cases
- **Read API Docs**: See inline documentation in source code
- **Join Community**: Contribute or ask questions on GitHub
- **Benchmark**: Test on standard datasets (FEVER, TruthfulQA)

## Additional Resources

- [CLI Usage Guide](CLI_USAGE.md)
- [Error Handling Guide](ERROR_HANDLING.md)
- [Configuration Reference](../config/README.md)
- [Contributing Guide](../CONTRIBUTING.md)
