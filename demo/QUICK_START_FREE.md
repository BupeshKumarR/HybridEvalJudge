# üöÄ Quick Start: Free LLM Evaluation (5 Minutes)

## The Fastest Way to Professional LLM Evaluation

This is the **absolute fastest** way to get a professional multi-agent LLM evaluation system running on your laptop with **zero cost**.

---

## ‚ö° 3-Step Setup

### Step 1: Install Ollama (2 minutes)

Visit [https://ollama.ai](https://ollama.ai) and download for your OS.

Or use package managers:

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

### Step 2: Pull Models (3 minutes)

```bash
# Best 3 models for most laptops (16GB RAM)
ollama pull phi3
ollama pull llama3.2:3b
ollama pull qwen2.5:3b
```

**Or use auto-setup:**
```bash
python demo/setup.py
```

### Step 3: Run Demo (30 seconds)

```bash
python demo/demo.py
```

**Done!** You now have a professional LLM evaluation system.

---

## üìä What You Get

### Real Multi-Model Comparison
- Tests 2-3 local models simultaneously
- Generates actual AI responses (not simulated)
- Compares them objectively
- Shows which model performs best

### Professional Output
```
üèÜ Model Rankings:
   ü•á 1. phi3: 87.3/100 (Confidence: 0.91)
   ü•à 2. llama3.2:3b: 82.1/100 (Confidence: 0.88)
   ü•â 3. qwen2.5:3b: 79.5/100 (Confidence: 0.85)

üéØ RECOMMENDATION:
   Best Model: phi3
   Verdict: APPROVED ‚úÖ
```

### Zero Cost
- No API keys
- No subscriptions
- Completely free forever
- Runs offline

---

## üíª System Requirements

| RAM | Models | Performance |
|-----|--------|-------------|
| 8GB | phi3 + llama3.2:1b | Good |
| 16GB | phi3 + llama3.2:3b + qwen2.5:3b | Excellent |
| 32GB+ | All above + mistral | Production-grade |

---

## üéØ Perfect For

### Portfolio Projects
Show employers you can:
- Build multi-agent AI systems
- Evaluate LLM quality
- Compare models objectively
- Create professional demos

### Resume Highlights
- "Built multi-agent LLM evaluation system"
- "Implemented hybrid AI assessment pipeline"
- "Developed zero-cost AI quality assurance tool"
- "Created local model comparison framework"

### GitHub Showcase
- Professional documentation ‚úÖ
- Working demos ‚úÖ
- Real evaluation results ‚úÖ
- Zero dependencies on paid APIs ‚úÖ

---

## üîß Customization (Optional)

### Test Your Own Questions

Edit `demo/demo.py` line ~150:

```python
question = "Your custom question here"
reference = "Your trusted reference information"
```

### Add More Models

```bash
ollama pull mistral      # 7B model (if 16GB+ RAM)
ollama pull codellama    # For code evaluation
ollama pull gemma:2b     # Very small model
```

They'll automatically appear in the demo!

---

## üìà Expected Results

### Typical Scores (Medical Q&A)

| Model | Accuracy | Speed | Memory |
|-------|----------|-------|--------|
| Phi-3 Mini | 85-90/100 | Fast | 2.3GB |
| Llama 3.2 3B | 80-85/100 | Fast | 1.9GB |
| Qwen2.5 3B | 75-85/100 | Fast | 1.9GB |
| Mistral 7B | 85-95/100 | Medium | 4.1GB |

### Common Issues Found
- Missing symptoms (medium severity)
- Incomplete explanations (low severity)
- Overly technical language (low severity)
- Occasional inaccuracies (high severity)

---

## üêõ Troubleshooting

### "Ollama not found"
```bash
# Check installation
which ollama

# Reinstall from https://ollama.ai
```

### "Model not found"
```bash
# List installed models
ollama list

# Pull missing model
ollama pull phi3
```

### "Out of memory"
```bash
# Use smaller model
ollama pull llama3.2:1b

# Or close other applications
```

### "Demo fails"
```bash
# Install dependencies
pip install psutil

# Test Ollama directly
ollama run phi3 "Hello"
```

---

## üìö Learn More

- **Complete Guide**: [FREE_SETUP_GUIDE.md](FREE_SETUP_GUIDE.md)
- **All Demos**: [README.md](README.md)
- **Main Docs**: [../README.md](../README.md)

---

## üéâ Success!

You now have:
- ‚úÖ Professional LLM evaluation system
- ‚úÖ Multiple free models running locally
- ‚úÖ Real comparison and ranking
- ‚úÖ Portfolio-ready project
- ‚úÖ Zero ongoing costs

**Total setup time: 5 minutes**  
**Total cost: $0**  
**Professional value: Priceless** üíé

---

## üöÄ Next Steps

1. **Today**: Run the demo with different questions
2. **This Week**: Customize for your domain
3. **This Month**: Add to your portfolio/GitHub
4. **Share**: Show it off on LinkedIn!

---

*This is the fastest way to get professional LLM evaluation running on your laptop!* ‚ö°
