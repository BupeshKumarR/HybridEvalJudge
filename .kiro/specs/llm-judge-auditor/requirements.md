# Requirements Document

## Introduction

The Hybrid LLM Evaluation Toolkit is an open-source system that combines multiple evaluation approaches to assess LLM outputs for factual accuracy, hallucinations, and bias. The system integrates: (1) specialized fact-checking models (e.g., MiniCheck, HHEM) for high-accuracy statement-level verification, (2) an ensemble of local judge LLMs (e.g., LLaMA 3, Mistral 7B) for broad coverage evaluation, (3) retrieval-augmented verification with external knowledge sources, and (4) research-backed hallucination quantification metrics (MiHR, MaHR, FactScore, Consensus F1, Fleiss' Kappa, uncertainty quantification). This hybrid architecture leverages the precision of fine-tuned verifiers, the flexibility of LLM judges, and rigorous statistical metrics to provide comprehensive, quantified hallucination detection. The toolkit is model-agnostic, domain-general, and runs entirely locally without requiring paid APIs or cloud compute.

## Glossary

- **Evaluation Toolkit**: The complete hybrid system that orchestrates specialized verifiers, judge model ensembles, and retrieval components
- **Specialized Verifier**: A small fine-tuned model (e.g., MiniCheck 770M, HHEM <1B) trained specifically for fact-checking and consistency verification
- **Judge Model Ensemble**: A collection of 2-3 local LLMs (e.g., LLaMA 3-8B, Mistral-7B, Phi-3) that evaluate outputs using structured prompts
- **Candidate Output**: The text generated by another LLM that is being evaluated for accuracy and quality
- **Source Text**: The reference document or context against which candidate outputs are verified
- **Hallucination**: False or unsupported claims in LLM output that are not grounded in the source text or known facts
- **Statement-Level Verification**: Fine-grained fact-checking that evaluates individual claims within a candidate output
- **Factual Accuracy Score**: A numerical rating (0-100) indicating how well the candidate output adheres to verifiable facts
- **Bias Detection**: The process of identifying stereotyped, harmful, or inconsistent language in model outputs
- **Chain-of-Thought Verification**: A reasoning approach where judge models explain their evaluation step-by-step
- **Retrieval Component**: A system that fetches relevant passages from knowledge bases to augment evaluation context
- **Knowledge Base**: External factual sources (e.g., Wikipedia dumps, FEVER dataset) used for fact-checking
- **Ensemble Aggregation**: The process of combining scores from multiple judge models into a final consensus evaluation
- **Multi-Stage Pipeline**: An evaluation workflow that applies specialized verifiers first, then broader LLM judges for comprehensive assessment
- **Micro Hallucination Rate (MiHR)**: Fraction of claims in a response that are unsupported (unsupported_claims / total_claims)
- **Macro Hallucination Rate (MaHR)**: Fraction of responses containing any hallucination (responses_with_hallucinations / total_responses)
- **FactScore**: Factual precision metric (verified_claims / total_claims)
- **Consensus F1**: F1 score combining precision (claims supported by other models) and recall (inclusion of consensus claims)
- **Fleiss' Kappa (κ)**: Inter-rater agreement statistic: κ = (Po - Pe) / (1 - Pe)
- **Shannon Entropy**: Uncertainty measure: H(p) = -Σ pᵢ log pᵢ
- **Epistemic Uncertainty**: Model uncertainty from lack of knowledge (variance across inference samples)
- **Aleatoric Uncertainty**: Inherent data noise (expected variance within samples)
- **Atomic Claim**: Single verifiable factual statement extracted from a response
- **Claim Verification Matrix**: Structure tracking which claims are supported by which models/judges
- **False Acceptance Rate**: Fraction of non-existent entity queries where model fails to abstain
- **Hallucination Profile**: Comprehensive report with all quantified metrics

## Requirements

### Requirement 1

**User Story:** As a developer, I want to initialize the hybrid evaluation toolkit with specialized verifiers and judge model ensembles, so that I can leverage multiple evaluation approaches without requiring cloud APIs or GPU hardware.

#### Acceptance Criteria

1. WHEN the Evaluation Toolkit starts, THE Evaluation Toolkit SHALL load the configured Specialized Verifier models from local storage using CPU or available GPU resources
2. WHEN the Evaluation Toolkit starts, THE Evaluation Toolkit SHALL load 2-3 Judge Models for the ensemble from local storage
3. WHERE quantization is enabled, THE Evaluation Toolkit SHALL apply 8-bit quantization to all models to reduce memory usage
4. WHEN any model fails to load, THE Evaluation Toolkit SHALL report a clear error message indicating which model failed and the failure reason
5. WHEN all models load successfully, THE Evaluation Toolkit SHALL verify each model is ready for inference before accepting evaluation requests

### Requirement 2

**User Story:** As a researcher, I want to use the multi-stage pipeline to evaluate candidate outputs for factual accuracy, so that I can leverage both specialized verifiers and judge ensembles for comprehensive hallucination detection.

#### Acceptance Criteria

1. WHEN a Source Text and Candidate Output are provided, THE Evaluation Toolkit SHALL first apply Statement-Level Verification using the Specialized Verifier
2. WHEN the Specialized Verifier completes, THE Evaluation Toolkit SHALL extract individual claim verdicts (supported, refuted, not enough info)
3. WHEN statement-level results are obtained, THE Evaluation Toolkit SHALL construct evaluation prompts for each Judge Model in the ensemble
4. WHEN each Judge Model processes its prompt, THE Evaluation Toolkit SHALL extract a Factual Accuracy Score between 0 and 100 with explanations
5. WHEN all judge models complete, THE Evaluation Toolkit SHALL aggregate scores using Ensemble Aggregation to produce a final consensus score
6. WHEN hallucinations are detected by any component, THE Evaluation Toolkit SHALL compile a unified list of unsupported claims with evidence from both verifiers and judges

### Requirement 3

**User Story:** As a content moderator, I want to detect bias and harmful language in model outputs, so that I can ensure fairness and safety in AI-generated content.

#### Acceptance Criteria

1. WHEN a Candidate Output is submitted for bias evaluation, THE Evaluation System SHALL construct a bias detection prompt
2. WHEN the Judge Model identifies stereotyped or harmful language, THE Evaluation System SHALL flag specific phrases with explanations
3. WHEN testing consistency across demographic variations, THE Evaluation System SHALL compare outputs for equivalent queries with different demographic attributes
4. WHEN bias is detected, THE Evaluation System SHALL assign a bias severity rating (low, medium, high)

### Requirement 4

**User Story:** As a data scientist, I want to perform pairwise ranking of multiple candidate outputs, so that I can determine which model produces more accurate results.

#### Acceptance Criteria

1. WHEN two Candidate Outputs are provided for the same Source Text, THE Evaluation System SHALL construct a pairwise comparison prompt
2. WHEN the Judge Model completes the comparison, THE Evaluation System SHALL identify which Candidate Output is more factually correct
3. WHEN the comparison completes, THE Evaluation System SHALL provide reasoning explaining the ranking decision
4. WHEN the two Candidate Outputs are equivalent in quality, THE Evaluation System SHALL indicate a tie with justification

### Requirement 5

**User Story:** As an educator, I want to batch process multiple evaluation requests, so that I can efficiently assess many student-generated answers at once.

#### Acceptance Criteria

1. WHEN a batch of evaluation requests is submitted, THE Evaluation System SHALL process each request sequentially
2. WHEN processing each request in the batch, THE Evaluation System SHALL generate individual evaluation results
3. WHEN the batch completes, THE Evaluation System SHALL aggregate scores and produce summary statistics (mean, median, distribution)
4. WHEN any request in the batch fails, THE Evaluation System SHALL log the error and continue processing remaining requests
5. WHEN batch processing completes, THE Evaluation System SHALL save results to a structured output file (JSON or CSV)

### Requirement 6

**User Story:** As a researcher, I want to integrate retrieval-augmented verification with external knowledge bases and support zero-retrieval fallback, so that the toolkit can evaluate both grounded and creative outputs.

#### Acceptance Criteria

1. WHEN a Knowledge Base is configured, THE Evaluation Toolkit SHALL initialize the Retrieval Component with the knowledge source (e.g., Wikipedia dump, FEVER dataset)
2. WHEN evaluating a Candidate Output with retrieval enabled, THE Retrieval Component SHALL extract key claims from the Candidate Output
3. WHEN key claims are extracted, THE Retrieval Component SHALL retrieve relevant passages from the Knowledge Base for each claim
4. WHEN relevant passages are found, THE Evaluation Toolkit SHALL provide them to both the Specialized Verifier and Judge Model Ensemble as factual context
5. WHEN no relevant passages are found for a claim, THE Evaluation Toolkit SHALL flag the claim as unverifiable and proceed with evaluation using only the Source Text
6. WHEN retrieval is disabled or fails, THE Evaluation Toolkit SHALL operate in zero-retrieval fallback mode using commonsense-only hallucination detection
7. WHEN in zero-retrieval mode, THE Evaluation Toolkit SHALL evaluate internal consistency and plausibility of the Candidate Output without external grounding

### Requirement 7

**User Story:** As a developer, I want to customize evaluation prompts and criteria, so that I can adapt the judge to different domains and evaluation needs.

#### Acceptance Criteria

1. WHEN the Evaluation System initializes, THE Evaluation System SHALL load prompt templates from a configuration file
2. WHEN a custom evaluation criterion is specified (e.g., relevance, completeness, style), THE Evaluation System SHALL construct a prompt incorporating that criterion
3. WHEN multiple criteria are requested, THE Evaluation System SHALL generate separate scores for each criterion
4. WHEN prompt templates are modified, THE Evaluation System SHALL apply the updated templates without requiring code changes

### Requirement 8

**User Story:** As a quality assurance engineer, I want to generate comprehensive, transparent evaluation reports with full provenance, so that I can analyze model performance and ensure explainability.

#### Acceptance Criteria

1. WHEN evaluations complete, THE Evaluation Toolkit SHALL compile results into a structured report containing scores, explanations, and flagged issues
2. WHEN generating reports, THE Evaluation Toolkit SHALL include metadata (timestamp, model versions, evaluation parameters, retrieval status)
3. WHEN generating reports, THE Evaluation Toolkit SHALL include retrieval provenance showing which Knowledge Base passages were used for each claim
4. WHEN generating reports, THE Evaluation Toolkit SHALL include chain-of-thought reasoning from each Judge Model explaining score decisions
5. WHEN generating reports, THE Evaluation Toolkit SHALL include confidence levels and model disagreement metrics for the ensemble
6. WHEN generating reports, THE Evaluation Toolkit SHALL list individual verdicts from each Judge Model alongside the consensus score
7. WHEN multiple evaluations are performed, THE Evaluation Toolkit SHALL support exporting reports in JSON, CSV, and human-readable text formats
8. WHEN hallucinations are detected, THE Evaluation Toolkit SHALL categorize them by type (factual error, unsupported claim, temporal inconsistency, numerical error)

### Requirement 9

**User Story:** As a system administrator, I want the evaluation system to handle errors gracefully, so that failures in one evaluation do not crash the entire system.

#### Acceptance Criteria

1. WHEN the Judge Model produces malformed output, THE Evaluation System SHALL attempt to parse partial results and log the parsing error
2. WHEN inference times out, THE Evaluation System SHALL terminate the request and return a timeout error with the partial response
3. WHEN memory limits are exceeded, THE Evaluation System SHALL clear cached data and retry the evaluation with reduced batch size
4. WHEN an unrecoverable error occurs, THE Evaluation System SHALL save the current state and provide diagnostic information for debugging

### Requirement 10

**User Story:** As a machine learning engineer, I want to validate the hybrid toolkit's reliability using comprehensive metrics and adversarial testing, so that I can ensure consistent, unbiased, and robust evaluations across all components.

#### Acceptance Criteria

1. WHEN evaluating the same Candidate Output multiple times, THE Evaluation Toolkit SHALL produce ensemble scores with variance below 5 points
2. WHEN the order of Candidate Outputs in pairwise comparison is reversed, THE Evaluation Toolkit SHALL maintain consistent ranking decisions with symmetric scores
3. WHEN testing on benchmark datasets (e.g., FEVER, TruthfulQA), THE Evaluation Toolkit SHALL report accuracy, precision, and recall for hallucination detection
4. WHEN computing inter-model agreement, THE Evaluation Toolkit SHALL calculate Cohen's kappa for the Judge Model Ensemble
5. WHEN evaluating pairwise rankings against ground truth, THE Evaluation Toolkit SHALL compute Kendall's Tau or Spearman correlation coefficients
6. WHEN demographic attributes in inputs are varied, THE Evaluation Toolkit SHALL detect and report any inconsistent scoring patterns across the ensemble
7. WHEN adversarial variants are generated (e.g., subtle date or location changes), THE Evaluation Toolkit SHALL detect the introduced hallucinations or contradictions
8. WHEN adversarial inputs with flipped facts are provided, THE Evaluation Toolkit SHALL flag them as inconsistent with the Source Text

### Requirement 11

**User Story:** As a researcher, I want to configure the ensemble aggregation strategy, so that I can experiment with different methods of combining judge model outputs.

#### Acceptance Criteria

1. WHEN the Evaluation Toolkit initializes, THE Evaluation Toolkit SHALL load the configured Ensemble Aggregation strategy (mean, median, weighted average, majority vote)
2. WHEN using weighted average aggregation, THE Evaluation Toolkit SHALL apply configured weights to each Judge Model's score
3. WHEN judge models disagree significantly (variance > 20 points), THE Evaluation Toolkit SHALL flag the evaluation as low-confidence
4. WHEN aggregation completes, THE Evaluation Toolkit SHALL report individual judge scores alongside the final consensus score

### Requirement 12

**User Story:** As a developer, I want to train or fine-tune a specialized verifier model, so that I can create domain-specific fact-checkers with high accuracy.

#### Acceptance Criteria

1. WHEN training data is provided (e.g., FEVER, synthetic fact-check pairs), THE Evaluation Toolkit SHALL support fine-tuning a small model (< 1B parameters) for statement verification
2. WHEN fine-tuning begins, THE Evaluation Toolkit SHALL use the provided training data to optimize the model for binary or ternary classification (supported/refuted/NEI)
3. WHEN fine-tuning completes, THE Evaluation Toolkit SHALL save the trained Specialized Verifier to local storage
4. WHEN evaluating the trained verifier on held-out data, THE Evaluation Toolkit SHALL report accuracy, precision, and recall metrics

### Requirement 13

**User Story:** As a quality assurance engineer, I want to compare the performance of specialized verifiers versus judge model ensembles, so that I can understand the strengths of each approach.

#### Acceptance Criteria

1. WHEN running evaluations with both components enabled, THE Evaluation Toolkit SHALL track separate metrics for the Specialized Verifier and Judge Model Ensemble
2. WHEN generating performance reports, THE Evaluation Toolkit SHALL include accuracy, latency, and confidence scores for each component
3. WHEN the Specialized Verifier and Judge Model Ensemble disagree, THE Evaluation Toolkit SHALL log the disagreement with both verdicts and reasoning
4. WHEN performance analysis completes, THE Evaluation Toolkit SHALL identify which component performs better on different types of claims (factual, temporal, numerical, etc.)

### Requirement 14

**User Story:** As a security researcher, I want to run adversarial robustness tests on the evaluation toolkit, so that I can verify it detects subtle manipulations and maintains consistency under adversarial conditions.

#### Acceptance Criteria

1. WHEN adversarial test mode is enabled, THE Evaluation Toolkit SHALL generate variants of input Candidate Outputs with subtle factual perturbations (e.g., changing dates, locations, numbers by small amounts)
2. WHEN adversarial variants are evaluated, THE Evaluation Toolkit SHALL detect and flag the introduced hallucinations or contradictions
3. WHEN testing pairwise ranking symmetry, THE Evaluation Toolkit SHALL evaluate both orderings (A vs B, B vs A) and verify that rankings are consistent
4. WHEN testing with contradictory claims, THE Evaluation Toolkit SHALL identify the contradiction and flag both statements
5. WHEN adversarial testing completes, THE Evaluation Toolkit SHALL generate a robustness report showing detection rates for different perturbation types
6. WHEN testing with biased input variations (e.g., demographic attribute changes), THE Evaluation Toolkit SHALL measure and report scoring consistency across variations

### Requirement 15

**User Story:** As a researcher, I want to compute Micro and Macro Hallucination Rates, so that I can quantify hallucination at claim-level and response-level granularity.

#### Acceptance Criteria

1. WHEN a response with extracted claims is evaluated, THE Evaluation Toolkit SHALL compute MiHR as unsupported_claims / total_claims
2. WHEN multiple responses are evaluated, THE Evaluation Toolkit SHALL compute MaHR as responses_with_hallucinations / total_responses
3. WHEN computing MiHR, THE Evaluation Toolkit SHALL classify each claim as supported, refuted, or unverifiable
4. WHEN MiHR or MaHR is computed, THE Evaluation Toolkit SHALL return values in range [0.0, 1.0]
5. WHEN a response contains zero extractable claims, THE Evaluation Toolkit SHALL return MiHR as None with a flag

### Requirement 16

**User Story:** As a data scientist, I want to compute FactScore and Consensus F1, so that I can measure factual precision and cross-model agreement.

#### Acceptance Criteria

1. WHEN a response is evaluated, THE Evaluation Toolkit SHALL extract atomic claims and compute FactScore as verified_claims / total_claims
2. WHEN multiple models respond to the same query, THE Evaluation Toolkit SHALL build a claim verification matrix
3. WHEN computing Consensus F1, THE Evaluation Toolkit SHALL calculate precision (model claims supported by others) and recall (consensus claims included)
4. WHEN precision and recall are computed, THE Evaluation Toolkit SHALL compute F1 = 2 × (precision × recall) / (precision + recall)
5. WHEN both precision and recall are zero, THE Evaluation Toolkit SHALL return F1 as 0.0

### Requirement 17

**User Story:** As a quality engineer, I want to measure inter-judge agreement using Fleiss' Kappa, so that I can assess evaluation reliability.

#### Acceptance Criteria

1. WHEN multiple judges evaluate the same claims, THE Evaluation Toolkit SHALL compute Fleiss' Kappa using κ = (Po - Pe) / (1 - Pe)
2. WHEN computing Kappa, THE Evaluation Toolkit SHALL calculate observed agreement (Po) and expected agreement (Pe)
3. WHEN Kappa is computed, THE Evaluation Toolkit SHALL provide interpretation (poor <0.2, fair 0.2-0.4, moderate 0.4-0.6, substantial 0.6-0.8, almost perfect >0.8)
4. WHEN fewer than two judges provide ratings, THE Evaluation Toolkit SHALL return Kappa as undefined with error

### Requirement 18

**User Story:** As a ML engineer, I want to quantify model uncertainty using Shannon entropy with epistemic/aleatoric decomposition, so that I can identify when models may hallucinate.

#### Acceptance Criteria

1. WHEN a model generates a response with probabilities, THE Evaluation Toolkit SHALL compute Shannon entropy H(p) = -Σ pᵢ log pᵢ
2. WHEN multiple inference samples are generated, THE Evaluation Toolkit SHALL compute epistemic uncertainty as variance across samples
3. WHEN multiple inference samples are generated, THE Evaluation Toolkit SHALL compute aleatoric uncertainty as expected variance within samples
4. WHEN uncertainty exceeds a threshold, THE Evaluation Toolkit SHALL flag the response as high-uncertainty
5. WHEN computing total uncertainty, THE Evaluation Toolkit SHALL return epistemic + aleatoric components

### Requirement 19

**User Story:** As a researcher, I want to generate hallucination profiles with all metrics, so that I can analyze model performance comprehensively.

#### Acceptance Criteria

1. WHEN evaluation completes, THE Evaluation Toolkit SHALL compile MiHR, MaHR, FactScore, F1, Kappa, and uncertainty into a hallucination profile
2. WHEN generating a profile, THE Evaluation Toolkit SHALL assign reliability classification (high, medium, low) based on thresholds
3. WHEN generating a profile, THE Evaluation Toolkit SHALL include claim-level analysis with disputed and consensus claims
4. WHEN generating a profile, THE Evaluation Toolkit SHALL serialize to JSON format
5. WHEN MiHR > 0.3 or Kappa < 0.4 or uncertainty > 0.8, THE Evaluation Toolkit SHALL flag as high risk

### Requirement 20

**User Story:** As a researcher, I want to measure False Acceptance Rate for abstention testing, so that I can evaluate model refusal behavior.

#### Acceptance Criteria

1. WHEN queries about non-existent entities are submitted, THE Evaluation Toolkit SHALL track abstention vs response
2. WHEN computing False Acceptance Rate, THE Evaluation Toolkit SHALL calculate failed_abstentions / total_nonexistent_queries
3. WHEN a model abstains appropriately, THE Evaluation Toolkit SHALL classify as correct refusal
4. WHEN a model generates content for non-existent entity, THE Evaluation Toolkit SHALL classify as false acceptance
