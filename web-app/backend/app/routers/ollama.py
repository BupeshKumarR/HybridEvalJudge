"""
Ollama router for LLM communication endpoints.
Provides streaming generation, model listing, and health checks.
"""
from fastapi import APIRouter, Depends, HTTPException, status, Query
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, List
import logging
import json

from ..services.ollama_service import (
    OllamaService,
    OllamaModel,
    Message,
    OllamaConnectionError,
    OllamaModelNotFoundError,
    OllamaServiceError,
    get_ollama_service,
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/ollama", tags=["ollama"])


# Request/Response schemas
class GenerateRequest(BaseModel):
    """Request schema for text generation."""
    question: str = Field(..., min_length=1, description="The question to ask the LLM")
    model: str = Field(default="llama3.2", description="Ollama model name")
    conversation_history: Optional[List[dict]] = Field(
        default=None,
        description="Previous messages for context"
    )
    system_prompt: Optional[str] = Field(
        default=None,
        description="Optional system prompt"
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="Sampling temperature"
    )
    top_p: float = Field(
        default=0.9,
        ge=0.0,
        le=1.0,
        description="Top-p sampling parameter"
    )


class OllamaModelResponse(BaseModel):
    """Response schema for a single Ollama model."""
    name: str
    size: Optional[int] = None
    digest: Optional[str] = None
    modified_at: Optional[str] = None


class ModelsListResponse(BaseModel):
    """Response schema for model list."""
    models: List[OllamaModelResponse]
    count: int


class HealthResponse(BaseModel):
    """Response schema for health check."""
    status: str
    ollama_available: bool
    host: str
    message: str


@router.post("/generate")
async def generate_stream(
    request: GenerateRequest,
    ollama_service: OllamaService = Depends(get_ollama_service)
):
    """
    Stream tokens from Ollama for a given question.
    
    This endpoint returns a Server-Sent Events (SSE) stream of tokens
    as they are generated by the LLM.
    
    Args:
        request: Generation request with question and parameters
        ollama_service: Ollama service instance
        
    Returns:
        StreamingResponse with SSE events containing tokens
        
    Raises:
        HTTPException: If Ollama is unavailable or model not found
    """
    # Convert conversation history to Message objects
    conversation_history = None
    if request.conversation_history:
        conversation_history = [
            Message(role=msg.get("role", "user"), content=msg.get("content", ""))
            for msg in request.conversation_history
        ]
    
    async def generate_tokens():
        """Generator that yields SSE events."""
        try:
            async for token in ollama_service.generate_stream(
                question=request.question,
                model=request.model,
                conversation_history=conversation_history,
                system_prompt=request.system_prompt,
                temperature=request.temperature,
                top_p=request.top_p
            ):
                # Format as SSE event
                event_data = json.dumps({"token": token, "done": False})
                yield f"data: {event_data}\n\n"
            
            # Send completion event
            completion_data = json.dumps({"token": "", "done": True})
            yield f"data: {completion_data}\n\n"
            
        except OllamaConnectionError as e:
            error_data = json.dumps({
                "error": "connection_error",
                "message": str(e),
                "suggestions": [
                    "Ensure Ollama is running: ollama serve",
                    "Check if Ollama is accessible at the configured host",
                    "Verify network connectivity"
                ]
            })
            yield f"data: {error_data}\n\n"
            
        except OllamaModelNotFoundError as e:
            error_data = json.dumps({
                "error": "model_not_found",
                "message": str(e),
                "suggestions": [
                    f"Pull the model: ollama pull {request.model}",
                    "Check available models with GET /api/v1/ollama/models"
                ]
            })
            yield f"data: {error_data}\n\n"
            
        except OllamaServiceError as e:
            error_data = json.dumps({
                "error": "service_error",
                "message": str(e)
            })
            yield f"data: {error_data}\n\n"
    
    return StreamingResponse(
        generate_tokens(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )


@router.get("/models", response_model=ModelsListResponse)
async def list_models(
    ollama_service: OllamaService = Depends(get_ollama_service)
):
    """
    Get list of available Ollama models.
    
    Args:
        ollama_service: Ollama service instance
        
    Returns:
        List of available models with metadata
        
    Raises:
        HTTPException: If Ollama is unavailable
    """
    try:
        models = await ollama_service.list_models()
        
        model_responses = [
            OllamaModelResponse(
                name=model.name,
                size=model.size,
                digest=model.digest,
                modified_at=model.modified_at
            )
            for model in models
        ]
        
        return ModelsListResponse(
            models=model_responses,
            count=len(model_responses)
        )
        
    except OllamaConnectionError as e:
        logger.warning(f"Ollama not available: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail={
                "error": "ollama_unavailable",
                "message": str(e),
                "suggestions": [
                    "Install Ollama from https://ollama.ai",
                    "Start Ollama with: ollama serve",
                    "Check if Ollama is running on the configured host"
                ]
            }
        )


@router.get("/health", response_model=HealthResponse)
async def check_health(
    ollama_service: OllamaService = Depends(get_ollama_service)
):
    """
    Check Ollama connection status.
    
    Args:
        ollama_service: Ollama service instance
        
    Returns:
        Health status with availability information
    """
    is_healthy = await ollama_service.check_health()
    
    if is_healthy:
        return HealthResponse(
            status="healthy",
            ollama_available=True,
            host=ollama_service.host,
            message="Ollama is running and accessible"
        )
    else:
        return HealthResponse(
            status="unhealthy",
            ollama_available=False,
            host=ollama_service.host,
            message="Ollama is not available. Please ensure Ollama is running."
        )
